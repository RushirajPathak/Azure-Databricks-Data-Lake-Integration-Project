# ğŸ§  Azure Databricks Data Lake Integration Project

## ğŸ“˜ Overview
This project demonstrates a **complete end-to-end data engineering pipeline** using **Microsoft Azure Databricks** and **Azure Data Lake Storage (ADLS Gen2)**.  
It covers data ingestion, transformation, and visualization using Apache Spark (PySpark) in the Databricks environment.  
The workflow follows an ELT (Extract, Load, Transform) pattern and is implemented using Databricks notebooks integrated with Azure services.

---

## ğŸš€ Project Objectives
- Set up and configure **Azure Databricks Workspace & Cluster**  
- Create and connect to **Azure Data Lake Storage (ADLS Gen2)**  
- Ingest raw data (`sales.csv`) from a **source container**  
- Perform data cleaning and transformation using **PySpark**  
- Write transformed data back to a **destination container** in ADLS  
- Register the processed data as a **Delta Table** in Databricks  
- Query and visualize aggregated results directly within Databricks  

---

## ğŸ—ï¸ Architecture Diagram
```text
Azure Portal
â”‚
â”œâ”€â”€ Resource Group (RG_DataLake)
â”‚    â”œâ”€â”€ Storage Account (ADLS Gen2)
â”‚    â”‚     â”œâ”€â”€ Container: source
â”‚    â”‚     â””â”€â”€ Container: destination
â”‚    â””â”€â”€ Databricks Workspace
â”‚          â”œâ”€â”€ Cluster (Single Node)
â”‚          â”œâ”€â”€ Notebook (PySpark code)
â”‚          â””â”€â”€ Metastore (Delta Table)
â”‚
â””â”€â”€ Visualization: Databricks Dashboards (Bar + Pie Charts)
